{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import models\n",
    "from utils.training import train_meta_model, train_gp\n",
    "from utils.data_utils import ctxt_trgt_split\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "tetouan_dataset = fetch_ucirepo(id=849) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "all_X = tetouan_dataset.data.features \n",
    "all_y = tetouan_dataset.data.targets \n",
    "\n",
    "# what features does the dataset have?\n",
    "print(all_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to normalised torch tensors, split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_times = pd.to_datetime(all_X['DateTime'], format=\"%m/%d/%Y %H:%M\")\n",
    "first_datetime = date_times[0]\n",
    "# convert time to days\n",
    "dt = torch.tensor(((date_times - first_datetime).dt.total_seconds() / (24 * 3600)).astype(float).values).unsqueeze(-1)\n",
    "temp_humid = torch.tensor(all_X[['Temperature', 'Humidity']].values)\n",
    "\n",
    "feb_idx = int((31 / 365) * dt.shape[0])\n",
    "mar_idx = int((59 / 365) * dt.shape[0])\n",
    "apr_idx = int((90 / 365) * dt.shape[0])\n",
    "\n",
    "# pre-march zones 1 and 2 datasets for training:\n",
    "# unnorm_X = torch.cat((dt, temp_humid), -1)[:feb_idx]\n",
    "# unnorm_ys = [torch.tensor(all_y[col].values)[:feb_idx] for col in all_y.columns]\n",
    "# pre-march datasets:\n",
    "unnorm_X = torch.cat((dt, temp_humid), -1)[:mar_idx]\n",
    "unnorm_ys = [torch.tensor(all_y[col].values)[:mar_idx] for col in all_y.columns]\n",
    "\n",
    "\n",
    "X_mean, X_std = unnorm_X.mean(0), unnorm_X.std(0)\n",
    "z1z2_unnorm_ys_cat = torch.cat((unnorm_ys[0], unnorm_ys[1]), dim=0)\n",
    "y_mean, y_std = z1z2_unnorm_ys_cat.mean(), z1z2_unnorm_ys_cat.std()\n",
    "\n",
    "X = (unnorm_X - X_mean) / X_std\n",
    "ys = [(unnorm_y - y_mean) / y_std for unnorm_y in unnorm_ys]\n",
    "\n",
    "training_tasks = [(X, ys[0]), (X, ys[1])]\n",
    "\n",
    "\n",
    "\n",
    "# zone 3 data split into pre march and march for context and target (extrapolation task)\n",
    "\n",
    "zone3_extrap_context = (X, ys[2])\n",
    "\n",
    "# target dataset for zone 3 extrapolation task:\n",
    "mar_unnorm_X = torch.cat((dt, temp_humid), -1)[mar_idx:apr_idx]\n",
    "mar_unnorm_y3 = torch.tensor(all_y.iloc[:,2].values)[mar_idx:apr_idx]\n",
    "\n",
    "z3_mar_X = (mar_unnorm_X - X_mean) / X_std\n",
    "z3_mar_y = (mar_unnorm_y3 - y_mean) / y_std\n",
    "\n",
    "zone3_extrap_target = (z3_mar_X, z3_mar_y)\n",
    "\n",
    "\n",
    "\n",
    "# zone 3 jan/feb data split into random context and target (interpolation task)\n",
    "\n",
    "z3_xc, z3_yc, z3_xt, z3_yt = ctxt_trgt_split(*zone3_extrap_context, ctxt_proportion_range=(0.49999, 0.50001))\n",
    "zone3_interp_context = (z3_xc, z3_yc)\n",
    "zone3_interp_target = (z3_xt, z3_yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the context and target data split for zone 3 extrapolation task\n",
    "\n",
    "plt.scatter(zone3_extrap_context[0][:,0], zone3_extrap_context[1])\n",
    "plt.scatter(zone3_extrap_target[0][:,0], zone3_extrap_target[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the context and target data split for zone 3 interpolation task\n",
    "\n",
    "plt.scatter(zone3_interp_context[0][:,0], zone3_interp_context[1])\n",
    "plt.scatter(zone3_interp_target[0][:,0], zone3_interp_target[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_chans = [128, 128, 128]\n",
    "kernel_size = 3\n",
    "Z_net_width = 128\n",
    "Z_net_depth = 3\n",
    "use_transformer = False\n",
    "num_induc = 256\n",
    "d_k = 16\n",
    "tetouan_grid_spacing = [1.25e-2, 5e-1, 5e-1]\n",
    "p1_raw = 1.0 # daily period\n",
    "p2_raw = 7.0 # weekly period\n",
    "nonlinearity = torch.nn.ReLU()\n",
    "\n",
    "prior = models.GPPrior(covariance_function='tetouan',\n",
    "                          num_inputs=3,\n",
    "                          p1=p1_raw / X_std[0],\n",
    "                          p2=p2_raw / X_std[0],\n",
    "                         )\n",
    "\n",
    "sgnp = models.SparseGaussianNeuralProcess(\n",
    "    x_dim=3,\n",
    "    num_inducing=num_induc,\n",
    "    likelihood=models.GaussianLikelihood(sigma_y = 0.02, train_sigma_y=True),\n",
    "    prior=prior,\n",
    "    d_k=d_k,\n",
    "    Z_net_width=Z_net_width,\n",
    "    Z_net_hidden_depth=Z_net_depth,\n",
    "    use_transformer=use_transformer,\n",
    "    nonlinearity=nonlinearity,\n",
    "    use_titsias=True,\n",
    ")\n",
    "\n",
    "# cnp = models.CNP(\n",
    "#     x_dim=3,\n",
    "#     deepset_dims = [Z_net_width] * Z_net_depth,\n",
    "#     decoder_dims = [Z_net_width] * Z_net_depth,\n",
    "#     nonlinearity=nonlinearity,\n",
    "# )\n",
    "\n",
    "# convcnp = models.ConvCNP(\n",
    "#     x_dim=3,\n",
    "#     cnn_hidden_chans=cnn_chans,\n",
    "#     cnn_kernel_size=kernel_size,\n",
    "#     tetouan_grid_spacing=tetouan_grid_spacing,\n",
    "#     nonlinearity=nonlinearity,\n",
    "# )\n",
    "\n",
    "# convgnp = models.ConvGNP(\n",
    "#     x_dim=3,\n",
    "#     cnn_hidden_chans=cnn_chans,\n",
    "#     cnn_kernel_size=kernel_size,\n",
    "#     d_k=d_k,\n",
    "#     nonlinearity=nonlinearity,\n",
    "#     tetouan_grid_spacing=tetouan_grid_spacing,\n",
    "# )\n",
    "\n",
    "# tnp_d = models.TNP(\n",
    "#     x_dim=3,\n",
    "#     num_layers=Z_net_depth,\n",
    "#     r_dim=Z_net_width,\n",
    "#     nonlinearity=nonlinearity\n",
    "# )\n",
    "\n",
    "# tnp_nd = models.TNP(\n",
    "#     x_dim=3,\n",
    "#     num_layers=Z_net_depth,\n",
    "#     r_dim=Z_net_width,\n",
    "#     non_diagonal=True,\n",
    "#     d_k=d_k,\n",
    "#     nonlinearity=nonlinearity,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################    SGNP    ################################################\n",
    "training_metrics = train_meta_model(\n",
    "    sgnp,\n",
    "    training_tasks,\n",
    "    training_steps=10_000,\n",
    "    batch_size=2,\n",
    "    learning_rate=1e-3,\n",
    "    final_learning_rate=5e-5,\n",
    "    num_samples=5,\n",
    "    loss_function='vi',\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, len(training_metrics), figsize=(3*len(training_metrics), 1))\n",
    "omitted_steps = 100\n",
    "for i, (key, value) in enumerate(training_metrics.items()):\n",
    "    axes[i].plot(value[omitted_steps:])\n",
    "    axes[i].set_xlabel(key)\n",
    "    axes[i].grid()\n",
    "plt.show()\n",
    "\n",
    "torch.save(sgnp, '../saved_models/tetouan-sgnp')\n",
    "# sgnp = torch.load('../saved_models/tetouan-sgnp', weights_only=False)\n",
    "\n",
    "\n",
    "##########################################    CNP    ################################################\n",
    "# training_metrics = train_meta_model(\n",
    "#     cnp,\n",
    "#     training_tasks,\n",
    "#     training_steps=10_000,\n",
    "#     batch_size=2,\n",
    "#     learning_rate=1e-3,\n",
    "#     final_learning_rate=5e-5,\n",
    "#     loss_function='npml',\n",
    "#     include_ctxt_in_trgt=True,\n",
    "#     ctxt_proportion_range=(0.2, 0.8),\n",
    "#     # task_subsample_fraction=0.2,\n",
    "# )\n",
    "\n",
    "# fig, axes = plt.subplots(1, len(training_metrics), figsize=(3*len(training_metrics), 1))\n",
    "# if not isinstance(axes, list):\n",
    "#     axes = [axes]\n",
    "# omitted_steps = 50\n",
    "# for i, (key, value) in enumerate(training_metrics.items()):\n",
    "#     axes[i].plot(value[omitted_steps:])\n",
    "#     axes[i].set_xlabel(key)\n",
    "#     axes[i].grid()\n",
    "# plt.show()\n",
    "\n",
    "# torch.save(cnp, '../saved_models/tetouan-cnp')\n",
    "\n",
    "\n",
    "##########################################    ConvCNP    ################################################\n",
    "# training_metrics = train_meta_model(\n",
    "#     convcnp,\n",
    "#     training_tasks,\n",
    "#     training_steps=10_000,\n",
    "#     batch_size=2,\n",
    "#     learning_rate=1e-3,\n",
    "#     final_learning_rate=5e-5,\n",
    "#     loss_function='npml',\n",
    "#     include_ctxt_in_trgt=True,\n",
    "#     ctxt_proportion_range=(0.2, 0.8),\n",
    "#     task_subsample_fraction=0.25,\n",
    "# )\n",
    "\n",
    "# fig, axes = plt.subplots(1, len(training_metrics), figsize=(3*len(training_metrics), 1))\n",
    "# if not isinstance(axes, list):\n",
    "#     axes = [axes]\n",
    "# omitted_steps = 50\n",
    "# for i, (key, value) in enumerate(training_metrics.items()):\n",
    "#     axes[i].plot(value[omitted_steps:])\n",
    "#     axes[i].set_xlabel(key)\n",
    "#     axes[i].grid()\n",
    "# plt.show()\n",
    "\n",
    "# torch.save(convcnp, '../saved_models/tetouan-convcnp')\n",
    "\n",
    "\n",
    "##########################################    ConvGNP    ################################################\n",
    "# training_metrics = train_meta_model(\n",
    "#     convgnp,\n",
    "#     training_tasks,\n",
    "#     training_steps=10_000,\n",
    "#     batch_size=2,\n",
    "#     learning_rate=1e-3,\n",
    "#     final_learning_rate=5e-5,\n",
    "#     loss_function='npml',\n",
    "#     include_ctxt_in_trgt=True,\n",
    "#     ctxt_proportion_range=(0.2, 0.8),\n",
    "#     task_subsample_fraction=0.2,\n",
    "# )\n",
    "\n",
    "# fig, axes = plt.subplots(1, len(training_metrics), figsize=(3*len(training_metrics), 1))\n",
    "# if not isinstance(axes, list):\n",
    "#     axes = [axes]\n",
    "# omitted_steps = 50\n",
    "# for i, (key, value) in enumerate(training_metrics.items()):\n",
    "#     axes[i].plot(value[omitted_steps:])\n",
    "#     axes[i].set_xlabel(key)\n",
    "#     axes[i].grid()\n",
    "# plt.show()\n",
    "\n",
    "# torch.save(convgnp, '../saved_models/tetouan-convgnp')\n",
    "\n",
    "\n",
    "##########################################    TNP-D    ################################################\n",
    "# training_metrics = train_meta_model(\n",
    "#     tnp_d,\n",
    "#     training_tasks,\n",
    "#     training_steps=10_000,\n",
    "#     batch_size=2,\n",
    "#     learning_rate=1e-3,\n",
    "#     final_learning_rate=5e-5,\n",
    "#     loss_function='npml',\n",
    "#     include_ctxt_in_trgt=True,\n",
    "#     ctxt_proportion_range=(0.2, 0.8),\n",
    "#     task_subsample_fraction=0.2,\n",
    "# )\n",
    "\n",
    "# fig, axes = plt.subplots(1, len(training_metrics), figsize=(3*len(training_metrics), 1))\n",
    "# if not isinstance(axes, list):\n",
    "#     axes = [axes]\n",
    "# omitted_steps = 50\n",
    "# for i, (key, value) in enumerate(training_metrics.items()):\n",
    "#     axes[i].plot(value[omitted_steps:])\n",
    "#     axes[i].set_xlabel(key)\n",
    "#     axes[i].grid()\n",
    "# plt.show()\n",
    "\n",
    "# torch.save(tnp_d, '../saved_models/tetouan-tnpd')\n",
    "\n",
    "\n",
    "##########################################    TNP-ND    ################################################\n",
    "# training_metrics = train_meta_model(\n",
    "#     tnp_nd,\n",
    "#     training_tasks,\n",
    "#     training_steps=10_000,\n",
    "#     batch_size=2,\n",
    "#     learning_rate=1e-3,\n",
    "#     final_learning_rate=5e-5,\n",
    "#     loss_function='npml',\n",
    "#     include_ctxt_in_trgt=True,\n",
    "#     ctxt_proportion_range=(0.2, 0.8),\n",
    "#     task_subsample_fraction=0.2,\n",
    "# )\n",
    "\n",
    "# fig, axes = plt.subplots(1, len(training_metrics), figsize=(3*len(training_metrics), 1))\n",
    "# if not isinstance(axes, list):\n",
    "#     axes = [axes]\n",
    "# omitted_steps = 50\n",
    "# for i, (key, value) in enumerate(training_metrics.items()):\n",
    "#     axes[i].plot(value[omitted_steps:])\n",
    "#     axes[i].set_xlabel(key)\n",
    "#     axes[i].grid()\n",
    "# plt.show()\n",
    "\n",
    "# torch.save(tnp_nd, '../saved_models/tetouan-tnpnd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c, y_c = *zone3_interp_context\n",
    "X_t, y_t = *zone3_interp_target\n",
    "\n",
    "################## Train SVGP ##########################\n",
    "# svgp_prior = models.GPPrior(covariance_function='tetouan',\n",
    "#                           num_inputs=3,\n",
    "#                           p1=p1_raw / X_std[0],\n",
    "#                           p2=p2_raw / X_std[0],\n",
    "#                          )\n",
    "# svgp = models.SparseVariationalGaussianProcess(x_dim=3,\n",
    "#                                                   num_inducing=256,\n",
    "#                                                   likelihood=sparse_gp.GaussianLikelihood(sigma_y = 0.02, train_sigma_y=True),\n",
    "#                                                   prior=svgp_prior,\n",
    "#                                                   use_titsias=True,\n",
    "#                                                   )\n",
    "# svgp.init_inducing_variables(X_c, y_c)\n",
    "\n",
    "# svgp_training_metrics = train_gp(\n",
    "#     svgp,\n",
    "#     X_c,\n",
    "#     y_c,\n",
    "#     epochs=10_000,\n",
    "#     learning_rate=1e-3,\n",
    "#     final_learning_rate=5e-5,\n",
    "#     svgp=True,\n",
    "#     num_samples=5,\n",
    "# )\n",
    "\n",
    "# fig, axes = plt.subplots(1, len(svgp_training_metrics), figsize=(3*len(svgp_training_metrics), 1))\n",
    "# omitted_steps = 50\n",
    "# for i, (key, value) in enumerate(svgp_training_metrics.items()):\n",
    "#     axes[i].plot(value[omitted_steps:])\n",
    "#     axes[i].set_xlabel(key)\n",
    "#     axes[i].grid()\n",
    "# plt.show()\n",
    "#############################################################\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # mult_predictive = convgnp(X_c, y_c, X_t)\n",
    "    # mult_predictive = tnp_nd(X_c, y_c, X_t)\n",
    "    predictive = torch.distributions.Normal(mult_predictive.mean, mult_predictive.variance.sqrt())\n",
    "\n",
    "    # predictive = tnp_d(X_t, X_c, y_c)\n",
    "    # predictive = cnp(X_t, X_c, y_c)\n",
    "    # predictive = convcnp(X_t, X_c, y_c)\n",
    "    # predictive = sgnp(X_t, X_c, y_c)\n",
    "    # predictive = svgp(X_t, X_c, y_c)\n",
    "\n",
    "root_n = y_t.shape[0].sqrt()\n",
    "print(root_n)\n",
    "print(\"ll: \", predictive.log_prob(y_t.squeeze()).mean(), predictive.log_prob(y_t.squeeze()).std())# / root_n)\n",
    "batch_ae = predictive.mean().squeeze() - y_t.squeeze()\n",
    "print(\"pred mean rmse: \", batch_ae.mean(), batch_ae.std())# / root_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c, y_c = *zone3_extrap_context\n",
    "X_t, y_t = *zone3_extrap_target\n",
    "\n",
    "################## Train SVGP ##########################\n",
    "# svgp_prior = models.GPPrior(covariance_function='tetouan',\n",
    "#                           num_inputs=3,\n",
    "#                           p1=p1_raw / X_std[0],\n",
    "#                           p2=p2_raw / X_std[0],\n",
    "#                          )\n",
    "# svgp = models.SparseVariationalGaussianProcess(x_dim=3,\n",
    "#                                                   num_inducing=256,\n",
    "#                                                   likelihood=sparse_gp.GaussianLikelihood(sigma_y = 0.02, train_sigma_y=True),\n",
    "#                                                   prior=svgp_prior,\n",
    "#                                                   use_titsias=True,\n",
    "#                                                   )\n",
    "# svgp.init_inducing_variables(X_c, y_c)\n",
    "\n",
    "# svgp_training_metrics = train_gp(\n",
    "#     svgp,\n",
    "#     X_c,\n",
    "#     y_c,\n",
    "#     epochs=10_000,\n",
    "#     learning_rate=1e-3,\n",
    "#     final_learning_rate=5e-5,\n",
    "#     svgp=True,\n",
    "#     num_samples=5,\n",
    "# )\n",
    "\n",
    "# fig, axes = plt.subplots(1, len(svgp_training_metrics), figsize=(3*len(svgp_training_metrics), 1))\n",
    "# omitted_steps = 50\n",
    "# for i, (key, value) in enumerate(svgp_training_metrics.items()):\n",
    "#     axes[i].plot(value[omitted_steps:])\n",
    "#     axes[i].set_xlabel(key)\n",
    "#     axes[i].grid()\n",
    "# plt.show()\n",
    "#############################################################\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # mult_predictive = convgnp(X_c, y_c, X_t)\n",
    "    # mult_predictive = tnp_nd(X_c, y_c, X_t)\n",
    "    predictive = torch.distributions.Normal(mult_predictive.mean, mult_predictive.variance.sqrt())\n",
    "\n",
    "    # predictive = tnp_d(X_t, X_c, y_c)\n",
    "    # predictive = cnp(X_t, X_c, y_c)\n",
    "    # predictive = convcnp(X_t, X_c, y_c)\n",
    "    # predictive = sgnp(X_t, X_c, y_c)\n",
    "    # predictive = svgp(X_t, X_c, y_c)\n",
    "\n",
    "root_n = y_t.shape[0].sqrt()\n",
    "print(root_n)\n",
    "print(\"ll: \", predictive.log_prob(y_t.squeeze()).mean(), predictive.log_prob(y_t.squeeze()).std())# / root_n)\n",
    "batch_ae = predictive.mean().squeeze() - y_t.squeeze()\n",
    "print(\"pred mean rmse: \", batch_ae.mean(), batch_ae.std())# / root_n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
